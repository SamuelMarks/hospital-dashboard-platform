"""
Benchmark Reporting Utility.

This script processes the raw CSV output generated by `run_benchmark.py` and
transforms it into a comprehensive Markdown report. It calculates aggregate
success metrics for different Prompt Engineering Strategies (Zero-Shot, CoT, RAG)
and identifies specific analytical themes where advanced strategies outperform the baseline.

The generated report (`BENCHMARK_RESULTS.md`) is designed to be directly
consumable by stakeholders to justify model/strategy selection.
"""

import csv
import sys
import os
import argparse
import statistics
from typing import List, Dict, Any, Tuple
from collections import defaultdict

# --- Data Structures ---


class BenchmarkResult:
  """
  Represents a single row from the raw CSV execution log.

  Attributes:
      theme (str): The analytical category (e.g. "Predictive Availability").
      strategy (str): The prompt engineering technique used.
      model (str): The specific LLM identifier.
      success (bool): Whether the generated SQL execution matched the Gold Standard.
      latency (int): Generation time in milliseconds.
  """

  def __init__(self, row: Dict[str, str]):
    """
    Initialize from a CSV row dictionary.

    Args:
        row (Dict[str, str]): Row data keyed by header name.
    """
    self.theme = row.get("Theme", "Unknown")
    self.strategy = row.get("Strategy", "Unknown")
    self.model = row.get("Model", "Unknown")
    # CSV writes booleans as "True"/"False" string
    self.success = row.get("Success") == "True"
    self.latency = int(row.get("LatencyMs", 0))


class StrategyStats:
  """
  Aggregator for performance metrics of a specific strategy.

  Attributes:
      total (int): Total number of test runs.
      wins (int): Number of successful runs.
      latencies (List[int]): Collection of response times.
  """

  def __init__(self) -> None:
    self.total = 0
    self.wins = 0
    self.latencies: List[int] = []

  @property
  def accuracy(self) -> float:
    """Returns percentage success rate (0-100)."""
    if self.total == 0:
      return 0.0
    return (self.wins / self.total) * 100

  @property
  def avg_latency(self) -> float:
    """Returns mean latency in ms."""
    if not self.latencies:
      return 0.0
    return statistics.mean(self.latencies)


# --- Core Logic ---


def load_results(csv_path: str) -> List[BenchmarkResult]:
  """
  Parses the input CSV file into objects.

  Args:
      csv_path (str): File path.

  Returns:
      List[BenchmarkResult]: List of parsed result objects.

  Raises:
      FileNotFoundError: If path is invalid.
  """
  if not os.path.exists(csv_path):
    raise FileNotFoundError(f"Input file not found: {csv_path}")

  results = []
  with open(csv_path, "r", encoding="utf-8") as f:
    reader = csv.DictReader(f)
    for row in reader:
      results.append(BenchmarkResult(row))
  return results


def aggregate_stats(results: List[BenchmarkResult]) -> Dict[str, StrategyStats]:
  """
  Groups results by Strategy.

  Args:
      results: The flat list of all test runs.

  Returns:
      Dict: Mapping of Strategy Name -> Stats Object.
  """
  stats = defaultdict(StrategyStats)
  for r in results:
    s = stats[r.strategy]
    s.total += 1
    if r.success:
      s.wins += 1
    s.latencies.append(r.latency)
  return stats


def analyze_improvements(results: List[BenchmarkResult]) -> List[Dict[str, Any]]:
  """
  Identifies specific Themes where Chain-of-Thought (CoT) succeeded
  while the Baseline (Zero-Shot) failed.

  Args:
      results: The flat list of runs.

  Returns:
      List[Dict]: A list of objects describing the improvement case.
      Format: [{'theme': '...', 'cot_model': '...', 'gap': True}]
  """
  # Group by Theme -> Strategy -> Result
  # Map: Theme -> Strategy -> List[BenchmarkResult]
  grouped = defaultdict(lambda: defaultdict(list))

  for r in results:
    grouped[r.theme][r.strategy].append(r)

  improvements = []

  for theme, strategies in grouped.items():
    # Get baseline results (Zero-Shot)
    zero_shot_runs = strategies.get("zero-shot", [])
    cot_runs = strategies.get("cot-macro", [])

    # Check if ALL zero-shots failed
    zero_shot_failed = len(zero_shot_runs) > 0 and all(not r.success for r in zero_shot_runs)

    # Check if ANY CoT succeeded
    cot_succeeded = any(r.success for r in cot_runs)

    if zero_shot_failed and cot_succeeded:
      best_cot = next(r for r in cot_runs if r.success)
      improvements.append({"theme": theme, "cot_model": best_cot.model})

  return sorted(improvements, key=lambda x: x["theme"])


def generate_markdown(stats: Dict[str, StrategyStats], improvements: List[Dict[str, Any]]) -> str:
  """
  Formats the analysis into a Markdown string using the requested template structure.

  Args:
      stats: Aggregated metrics.
      improvements: List of specific wins over baseline.

  Returns:
      str: The full markdown content.
  """
  lines = []
  lines.append("# LLM Benchmarking Report")
  lines.append(f"\n**Generated on:** {os.environ.get('BUILD_DATE', 'Local Execution')}")

  # 1. Summary Table
  lines.append("\n## 1. Strategy Performance Summary")
  lines.append("| Strategy | Accuracy (%) | Avg Latency (ms) | Samples |")
  lines.append("| :--- | :---: | :---: | :---: |")

  # Sort strategies for consistent output
  for strat in sorted(stats.keys()):
    s = stats[strat]
    lines.append(f"| **{strat}** | {s.accuracy:.1f}% | {s.avg_latency:.0f} | {s.total} |")

  # 2. Failure Analysis
  lines.append("\n## 2. Failure Analysis: CoT Improvements")
  lines.append(
    "The following themes failed consistently with **Zero-Shot** but were solved by **Chain-of-Thought (CoT)** logic:"
  )
  lines.append("")

  if not improvements:
    lines.append("*No distinct improvements detected (Baseline performed adequately or both failed).*")
  else:
    for item in improvements:
      lines.append(f"- **{item['theme']}**: Fixed by `{item['cot_model']}` using CoT.")

  return "\n".join(lines)


# --- CLI Entry Point ---


def main() -> None:
  """
  Main execution loop.
  Reads input CSV, processes data, and writes Markdown report.
  """
  parser = argparse.ArgumentParser(description="Generate Benchmark Report")
  parser.add_argument("--input", default="leaderboard.csv", help="Path to raw CSV results")
  parser.add_argument("--output", default="BENCHMARK_RESULTS.md", help="Path to output Markdown")
  args = parser.parse_args()

  try:
    data = load_results(args.input)
    stats = aggregate_stats(data)
    improvements = analyze_improvements(data)

    md_content = generate_markdown(stats, improvements)

    with open(args.output, "w", encoding="utf-8") as f:
      f.write(md_content)

    print(f"✅ Report generated: {args.output}")

  except Exception as e:
    print(f"❌ Error: {e}")
    sys.exit(1)


if __name__ == "__main__":
  main()
